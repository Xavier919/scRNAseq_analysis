{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f8e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 15:26:46.613038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 15:26:47.155547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from modules.utils import *\n",
    "from modules.deg_analysis import *\n",
    "from modules.visualize import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from modules.dataloader import PairedDataset\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from modules.mlp_model import MLP, SiameseMLP\n",
    "from modules.kan_model import DeepKAN, SiameseKAN\n",
    "import torch.nn as nn\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47709c5-2f35-45ad-b445-ff7922e4ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_tag_mapping = {'SampleTag17_flex':'WT-DMSO',\n",
    "#                       'SampleTag18_flex':'3xTg-DMSO',\n",
    "#                       'SampleTag19_flex':'WT-SCDi',\n",
    "#                       'SampleTag20_flex':'3xTg-SCDi',\n",
    "#                       'Undetermined':'Undetermined',\n",
    "#                       'Multiplet':'Multiplet'}\n",
    "# adata = anndata.read_h5ad(\"data/fede_count.h5ad\")\n",
    "# adata.obs['Sample_Tag'] = adata.obs['Sample_Tag'].map(sample_tag_mapping)\n",
    "# anno_df = pd.read_csv(\"data/fede_mapping.csv\", skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcabb4ee-a1bc-4496-8faf-8b410e70c5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "adata1 = anndata.read_h5ad(\"data/A_count.h5ad\")\n",
    "adata1.obs['Sample_Tag'] = 'LD_5xFAD'\n",
    "adata2 = anndata.read_h5ad(\"data/B_count.h5ad\")\n",
    "adata2.obs['Sample_Tag'] = \"LD_NC\"\n",
    "adata3 = anndata.read_h5ad(\"data/C_count.h5ad\")\n",
    "adata3.obs['Sample_Tag'] = \"run_5xFAD\"\n",
    "adata4 = anndata.read_h5ad(\"data/D_count.h5ad\")\n",
    "adata4.obs['Sample_Tag'] = \"run_NC\"\n",
    "adata = anndata.concat([adata1, adata2, adata3, adata4], axis=0)\n",
    "\n",
    "anno_df1 = pd.read_csv(\"data/A_mapping.csv\", skiprows=4)\n",
    "anno_df2 = pd.read_csv(\"data/B_mapping.csv\", skiprows=4)\n",
    "anno_df3 = pd.read_csv(\"data/C_mapping.csv\", skiprows=4)\n",
    "anno_df4 = pd.read_csv(\"data/D_mapping.csv\", skiprows=4)\n",
    "anno_df = pd.concat([anno_df1, anno_df2, anno_df3, anno_df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d09076-f7fa-466e-86ad-20edd42a6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = annotate_adata(adata, anno_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c3c763-2d21-42e5-a3e3-787b0bbe3f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "sc.pp.filter_cells(adata, min_genes=150)\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "adata.var['mt'] = adata.var_names.str.startswith('mt-')\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "adata.obs['high_mt'] = adata.obs['pct_counts_mt'] > 5\n",
    "adata = adata[~adata.obs['high_mt'], :]\n",
    "#adata = adata[adata.obs['Sample_Tag'] != \"Multiplet\", :]\n",
    "#adata = adata[:, ~adata.var['mt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d60e56-3636-4a59-ab0c-c0898dad7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:206: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "sc.pp.normalize_total(adata)\n",
    "#sc.pp.log1p(adata)\n",
    "#sc.pp.scale(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94458c32-f9c0-4b4a-bd00-785e188e6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = {category: idx for idx, category in enumerate(adata.obs['Sample_Tag'].unique())}\n",
    "adata.obs['target'] = adata.obs['Sample_Tag'].map(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbb726-d802-4290-a403-3e956c93d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e485a92b-ea77-4e7e-a11c-1b692540f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/client/projects/sc_alz/sc_env/lib/python3.10/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(adata.n_obs), test_size=0.2, random_state=42)\n",
    "adata_train = adata[train_indices].copy()\n",
    "adata_test = adata[test_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb46b114-55b5-4d03-8f16-96d8d8c43c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(adata_train.obs['target'].tolist())\n",
    "Y_test = np.array(adata_test.obs['target'].tolist())\n",
    "X_train = adata_train.X.toarray()\n",
    "X_test = adata_test.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96136cfb-3984-4658-985a-06287bfe76cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = PairedDataset(X_train, Y_train, 5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512)\n",
    "\n",
    "test_dataset = PairedDataset(X_test, Y_test, 5)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4b8c3f-f31d-49b9-b049-39b25ae1ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[-1]\n",
    "shared_layers = [4096,1024,256,32]\n",
    "base_net = MLP(input_dim, shared_layers).to(device)\n",
    "siamese_model = SiameseMLP(base_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4435e7e3-6854-4317-b331-60172b424890",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(siamese_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd308c3-ecb0-4a47-9e76-6893f92d6ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▍                        | 295/737 [00:38<00:57,  7.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m no_improvement_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msiamese_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m eval_model(siamese_model, test_loader, device, epoch)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/sc_alz/modules/utils.py:50\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     48\u001b[0m regularize_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     49\u001b[0m l1_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data_a, data_b), target \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     51\u001b[0m     data_a, data_b \u001b[38;5;241m=\u001b[39m data_a\u001b[38;5;241m.\u001b[39mto(device), data_b\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m     target  \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "best_accuracy = 0\n",
    "no_improvement_count = 0 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(siamese_model, train_loader, optimizer, device, epoch)\n",
    "    val_accuracy = eval_model(siamese_model, test_loader, device, epoch)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        no_improvement_count = 0  \n",
    "        torch.save(siamese_model.base_network.state_dict(), 'best_model.pth')\n",
    "        print(\"Model saved as best model\")\n",
    "    else:\n",
    "        no_improvement_count += 1  \n",
    "\n",
    "    if no_improvement_count >= 5:\n",
    "        print(\"No improvement in validation accuracy for 5 consecutive epochs. Training stopped.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21486b45-967b-4fba-9d54-a5aad886f307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a05f6-03f0-4bef-af60-96defae208d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f771cc18-5dcb-49d2-ae22-91c8b08b7e79",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m base_net \u001b[38;5;241m=\u001b[39m MLP(input_dim, shared_layers)\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m base_net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n\u001b[1;32m      7\u001b[0m base_net\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/projects/sc_alz/sc_env/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model.pth'"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[-1]\n",
    "shared_layers = [4096,1024,256,32]\n",
    "base_net = MLP(input_dim, shared_layers)\n",
    "model_path = 'best_model.pth'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "base_net.load_state_dict(checkpoint)\n",
    "base_net.to(device)\n",
    "\n",
    "base_net.eval()\n",
    "base_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9679abd-3399-411a-9213-3c521e5695a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(adata_test.X.toarray(), dtype=torch.float32)\n",
    "latents = base_net(data)\n",
    "latents_np = latents.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cb71a-620c-467c-8bb7-f0f5308af6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test.obsm['latents'] = latents_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d888d00-ee15-4c91-9cc0-b14c8126501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata_test, use_rep='latents')\n",
    "sc.tl.leiden(adata_test, resolution=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cc89b-5d8f-4eff-8adb-be76f65a6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ce5ab-4f59-48a3-8754-b9a1378d67de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_test, color=['leiden'], save='umap_leiden.png')\n",
    "sc.pl.umap(adata_test, color=['Sample_Tag'], save='umap_sampletag.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77391f8b-ed82-4944-a903-4f406e9d20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_unique_cell_type_names(adata_test, cluster_key='leiden', cluster_types=['class_name', 'subclass_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e788a-9c86-49e7-a6f9-6e9372872844",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_test, color=['cluster_subclass_name'], save='umap_all_groups.png', title=f'After QC - {adata_test.shape[0]} cells', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d84dc7-bc03-4cbc-9ea8-5874d96c3cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_test, color=['Sample_Tag'], save='umap_all_groups.png', title=f'After QC - {adata_test.shape[0]} cells', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dceed6-85f0-466a-aea3-a1cba7a186e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sample_tags = adata_test.obs['Sample_Tag'].unique()\n",
    "#plot_umap(adata_test, cluster_type='cluster_subclass_name', legend_fontsize=7, save_path='_sample_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130f124-9116-4cdc-a723-977d0295e479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a32ea-b635-4565-83e0-59bde8afe96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = adata_test.obs['Sample_Tag'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3f802-33ab-4e96-a95b-467c5a725a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_level, cluster_type = 'subclass_name', 'cluster_subclass_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674be122-2999-4b92-ba4e-26963ad1c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ditto_plot(adata_test, tags, class_level=class_level, cluster_type=cluster_type, min_cell=100, save_path='figures/dito_siamese.png')\n",
    "#create_ditto_plot(adata_test, [tags[0]], class_level=class_level, cluster_type=cluster_type, min_cell=100, save_path=None)\n",
    "#create_ditto_plot(adata_test, [tags[1]], class_level=class_level, cluster_type=cluster_type, min_cell=100, save_path=None)\n",
    "#create_ditto_plot(adata_test, [tags[2]], class_level=class_level, cluster_type=cluster_type, min_cell=100, save_path=None)\n",
    "#create_ditto_plot(adata_test, [tags[3]], class_level=class_level, cluster_type=cluster_type, min_cell=100, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a539d-12ec-46cc-9fe2-fea3dc41428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ebc14-2c2d-49c6-b467-ddb6f268f1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
